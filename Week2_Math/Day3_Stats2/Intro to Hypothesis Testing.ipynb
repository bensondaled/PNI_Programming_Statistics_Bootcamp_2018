{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Intro to Hypothesis Testing</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: our first hypothesis test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You find a brain in the human dissection lab, but you suspect it may belong to a new unknown species.\n",
    "\n",
    "You decide that mass of brain is the only important feature. So you will measure its mass, then compare it to an online database of all human brain masses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you measure the mass of the brain, and record it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_brain_mass = 1564.2 # grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you download your dataset and load it into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1031.03862494 1132.34383661 1188.58290064 ... 1325.03535947 1342.84315573\n",
      " 1578.74968743]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "brain_masses = np.load('human_brain_masses.npy')\n",
    "print(brain_masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Does our sample brain come from a different *population* than the brains in this list of human brain masses?__  \n",
    "*(it's worth noting, though okay to ignore for now, that we are treating these brain measurements as a __population__ and not a sample from a larger populationâ€“we'll deal with this later)*  \n",
    "\n",
    "How can we answer that question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Intuitive approach: \n",
    "\n",
    "Is our brain smaller than the largest human brain?  \n",
    "Is it larger than the smallest human brain?  \n",
    "Is it larger or smaller than the average human brain?  \n",
    "How many human brains are larger than our sample?  \n",
    "How many are smaller? </i>\n",
    "\n",
    "__Task__:  \n",
    "Answer these questions using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could ask hundreds of such questions. Visualization can help speed this up.\n",
    "\n",
    "__Task__:  \n",
    "(1) plot a histogram of the human brain mass dataset  \n",
    "(2) label the mean human brain and our sample brain on that histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's not possible to be *certain* about the answer to our question, we can judge *how likely* it is that our brain came from this population distribution.\n",
    "\n",
    "One measure of that is the fraction of brains that are __further from the mean__ than our sample is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "(1) Compute the *fraction* of human brains with a mass larger than our sample brain.  \n",
    "(2) Compute the *fraction* of human brains with a mass *further from the mean* than our sample brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could rephrase the latter fraction as \"the probability of finding a brain with a more outlying mass than our sample.\"  \n",
    "This is often referred to as a \"p-value.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value is a \"one-tailed\" value, and the second is a \"two-tailed\" value. Unless you have strong reason to believe your sample could not have been extreme in other direction (in our case, lighter), then it makes most sense to use a two-tailed test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "### Generalizing the approach\n",
    "\n",
    "If we can *assume* that the population underlying our data are *normally distributed*, then we can make that analysis very general.\n",
    "\n",
    "(Recall the <a href=\"https://en.wikipedia.org/wiki/Central_limit_theorem\">central limit theorem</a> for some basic intuition on the generality of normal distributions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why, let's visualize the basic properties of a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "xs = np.linspace(-4,4,1000)\n",
    "ys = norm.pdf(xs, 0, 1)\n",
    "\n",
    "fig,ax = pl.subplots()\n",
    "\n",
    "ax.plot(xs, ys, color='k', lw=2)\n",
    "\n",
    "for i,col in zip([3, 2.5, 2, 1], [.5, .4, .3, .2]):\n",
    "    use = (xs>0) & (xs<i)\n",
    "    ax.fill_between(xs[use], ys[use], color=pl.cm.Greys(col), lw=0)\n",
    "    ax.annotate(f'{i} std', [np.max(xs[use])+.15,np.min(ys[use])+.01], color=pl.cm.Greys(col), ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help illustrate the value of understanding this point, consider what happens when we choose to show our data in an arbitrary new unit of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axs = pl.subplots(2,1,figsize=(5,7), sharex=True)\n",
    "\n",
    "for ax,scale,unit in zip(axs, [1,0.035274], ['g','oz']):\n",
    "    ax.hist(brain_masses*scale, color='k', bins=500, histtype='step')\n",
    "    ax.set_xlabel(f'Brain mass ({unit})')\n",
    "    ax.set_yticks([])\n",
    "    ax.axvline(sample_brain_mass*scale, color='darkmagenta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are identical, but if we didn't know that, it would not be immediately obvious how much of an \"outlier\" each one is, because the units are different.  \n",
    "\n",
    "To get around this, we normalize our measurements by subtracting the mean and dividing by standard deviation to produce a __z-score__.  \n",
    "<br>\n",
    "<center>$z = \\frac{\\text{sample value - population mean}}{\\text{population standard deviation}}$</center>\n",
    "\n",
    "<br>\n",
    "<center>$z = \\frac{x-\\mu}{\\sigma}$</center>\n",
    "\n",
    "\n",
    "where $x$ is our sample value, $\\mu$ is the population mean, and $\\sigma$ is the population standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task__: compute the z-score for every brain in our dataset, and for the sample brain, and display them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of values once z-scored is called the <u>*standard* normal distribution</u>, and it has a mean of 0 and a standard deviation of 1. \n",
    "\n",
    "We can use this system to express in general terms (a) how much of an outlier a data point is (ex. \"1.2 standard deviations from the mean\"), and (b) how likely that data point was to have arisen from the population (p-value).\n",
    "\n",
    "We can use existing packages in Python to determine the probability of observing any given data point in a normal distribution. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm # norm is the normal distribution\n",
    "\n",
    "z_score = (sample_brain_mass-mean_human_brain_mass) / np.std(brain_masses, ddof=1)\n",
    "p = norm.pdf(z_score) # probability density function: returns the fraction of the distribution that is greater than the input value \n",
    "# (and it uses the standard normal distribution by default)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "### Putting it all together: conducting a hypothesis test\n",
    "\n",
    "__Null hypothesis__: our sample brain comes from the population of human brains  \n",
    "__Alternative hypothesis__: our sample brain comes from a separate population of brains  \n",
    "\n",
    "__Compute z-score__  \n",
    "__Compute p-value__  \n",
    "__Compute \"effect size\"__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What reasonable conclusions can make from this analysis? \n",
    "\n",
    "*What does the p-value tell us? What does it not tell us?*\n",
    "\n",
    "*One caveat*: This is also quite an odd scenario in reality, because we rarely have information about full *populations* and single data points to compare them to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: dealing with samples of multiple observations\n",
    "\n",
    "We often want to know about a *group of sampled points* as opposed to a single point.  \n",
    "A common example would be to ask \"does this sample come from a population with a mean of X?\" \n",
    "\n",
    "This has a similar flavor to our first example question. It's a bit more complex, but the same logic applies.\n",
    "\n",
    "Let's begin by loading in a similar dataset, but this time consisting of a set of sampled observations.  \n",
    "Suppose each sample represents the mean firing rate of a single neuron.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 19.08765187  68.65806486  33.56354929  32.49577301  30.48491205\n",
      "   9.27890057  27.96165612  -3.17118488  19.24172174  35.57012565\n",
      "   3.46882322  33.69680934  29.00681338  26.24718219  34.13053343\n",
      "  82.60800516  72.86523956  63.62233261  53.87813344  -2.98364416\n",
      "   2.923537    52.54128835   9.43490808  61.32799195   4.09816448\n",
      "  88.87137507  36.40587655  46.21497085 -19.43758789  20.50959796\n",
      " -44.17382065  22.16441264  -1.10324205 -13.17386638  42.25494227\n",
      "  27.17121254  94.43270511 -41.27862449  35.76603664  42.72439828\n",
      " -17.77928443 -58.33710308  42.46037535  51.20356776  65.45432975\n",
      "  37.14975646   0.503567    54.79503172  34.73825892 -24.52529967\n",
      "  13.70100255  14.9085       4.30102928 -26.74032759  63.57866916\n",
      " 100.64613485  53.43042129  59.48199558 -22.85693732  13.48268966\n",
      " -27.91227646  -0.54621774  56.90457851  24.52251321  58.36478292\n",
      "  49.79139193  38.61330352  36.20647124  -0.23270032  55.51730976\n",
      "   4.61814591  74.60904414   6.19978153  24.11885351 -28.61337162\n",
      "  -1.32663502  40.71207735  76.76817256  22.92906206  16.21432084\n",
      " -24.16177401  26.53378497  12.00422091  64.31092013   4.1792906\n",
      "  95.53059147  46.54118424  39.85992124  35.67754714  46.45533629\n",
      "  -0.7171342   81.79539757 -20.42160518   5.68593726  -9.45393747\n",
      " -23.88626546  36.00012173 -14.84516246  28.89370666  52.8690389\n",
      "  74.3129809   33.03282881  24.19591884  24.53749305 -16.16507378\n",
      "  68.80438552  11.82147498  -1.92261506  44.08131527  32.46683999\n",
      "  13.76677877   5.5412133   48.21830143  45.77237544  20.35519523\n",
      "  41.23526278  33.22448438  42.12667807  40.43332464  50.56616734]\n"
     ]
    }
   ],
   "source": [
    "sample = np.load('sample.npy')\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that these neurons come from a neuronal population that has a mean firing rate of 30 Hz.\n",
    "\n",
    "Null hypothesis: the sample comes from a population with a mean of 30.  \n",
    "Alternative hypothesis: the sample comes from a population with a mean not equal to 30.\n",
    "\n",
    "__Task__: inspect these data and speculate on how you might answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.030809250655714\n"
     ]
    }
   ],
   "source": [
    "# inspect here\n",
    "print(np.mean(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A naÃ¯ve idea:* compute the sample mean, and if it differs from 30, deem the alternative hypothesis correct.\n",
    "\n",
    "This and many other simple approaches don't work because we don't know __what to expect by chance when sampling a set of points from a population__. In other words, we haven't specified our *null distribution* (i.e. the one we conveniently had available in the previous example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual demonstration\n",
    "\n",
    "fig,ax = pl.subplots()\n",
    "\n",
    "xs = np.arange(15,45,.01)\n",
    "ys = norm.pdf(xs, 30, 5)\n",
    "ax.plot(xs, ys, color='black', label='Population')\n",
    "ax.axvline(30, lw=4, color='k')\n",
    "ax.legend(loc='upper left')\n",
    "pl.waitforbuttonpress()\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "for i,col in enumerate(['maroon','darkorange','forestgreen','steelblue','violet','grey']):\n",
    "    sample = np.random.normal(30, 5, size=20)\n",
    "    ax2.hist(sample, bins=8, density=True, histtype='step', color=col, alpha=.5, label=f'Sample {i}')\n",
    "    ax2.axvline(sample.mean(), lw=2, color=col)\n",
    "    ax2.legend(loc='upper right')\n",
    "    pl.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure this out, we could look into probability or statistics theory, but we can also build some intuition ourselves first.\n",
    "\n",
    "Let's create a \"known\" population and sample from it many times, then analyze what we observe.\n",
    "\n",
    "This will allow us to build some intuition about what happens when we sample, and what to expect about a population based on a single sample that came from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Our \"population\": mean=30, std=10')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,std = 30,10\n",
    "\n",
    "fig,ax = pl.subplots()\n",
    "\n",
    "population = np.random.normal(mean,std,size=100000)\n",
    "# show this histogram, and say: if we can somehow infer this exact population properties using just a single sample,\n",
    "# then we can do our original test -- that's what a one-sample t-test ends up being\n",
    "ax.hist(population, color='k', histtype='step', bins=200, density=True)\n",
    "xs = np.arange(20,40,.01)\n",
    "ys = norm.pdf(xs,mean,std)\n",
    "ax.plot(xs,ys,color='k')\n",
    "ax.set_title(f'Our \"population\": mean={mean}, std={std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to learn what we can expect when sampling from this population.\n",
    "\n",
    "todo: note \"with/without replacement\" - could be a chance to define replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__ from our \"population\", collect 10000 random samples each containing 50 observations.\n",
    "\n",
    "todo: be extremely clear about samples vs observations vs etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10000 # number of times to sample the population\n",
    "n = 50 # number of observations to take in each sample\n",
    "samples = np.array([np.random.choice(population, size=n) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson #1 of sampling: how do the means of our samples relate to mean of the population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(91.5694,0.5,'Frequency')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig,axs = pl.subplots(2,1,sharex=True,gridspec_kw=dict(hspace=.4))\n",
    "cols = ['red','orange','gold','green','blue','violet','grey','steelblue','pink']\n",
    "\n",
    "pl.waitforbuttonpress()\n",
    "\n",
    "for si,(sample,col) in enumerate(zip(samples,cols)):\n",
    "    axs[0].hist(sample, histtype='step', color=col)\n",
    "    axs[0].axvline(sample.mean(), color=col)\n",
    "    axs[1].axvline(sample.mean(), color=col, alpha=.5)\n",
    "    axs[0].set_xlabel('Observed value')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Each histogram is a sample of observations\\nWe get one of these when we do an experiment in real life')\n",
    "    axs[1].set_xlabel('Mean of observations in a given sample')\n",
    "    pl.pause(1)\n",
    "axs[1].hist(samples.mean(axis=1), bins=50, histtype='step', color='k', lw=3)\n",
    "axs[1].set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we're beginning to get a sense of how much we can expect our sample mean to reflect the true mean.\n",
    "\n",
    "This distribution is our \"null distribution.\"\n",
    "\n",
    "todo: comment on specific details: is it symmetric? etc.\n",
    "\n",
    "If we could characterize the center and shape of our null distribution (i.e. its mean and standard deviation), we could compute the probability of observing a single sample from it, as we did in the simpler case before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson #2 of sampling: how does `n` of our samples relate to our estimate of the population mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c4b4a0fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#task\n",
    "\n",
    "for n in range(10,200,50):\n",
    "    samples = np.array([np.random.choice(population, size=n) for i in range(N)])\n",
    "    axs[1].hist(samples.mean(axis=1), bins=50, histtype='step', label=f'n={n}', lw=3, color=pl.cm.Greys(n/200+.1))\n",
    "axs[1].legend(title='n: # of observations in a sample', fontsize='x-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson #3 of sampling: How can we use one sample to infer our null distribution?\n",
    "\n",
    "It turns out that our null distribution of sample means can be inferred from:  \n",
    "* (1) our sample mean\n",
    "* (2) our sample standard deviation ($s$)\n",
    "* (3) our sample size ($n$)\n",
    "\n",
    "Specifically, our null distribution is estimated as a normal distribution with the same mean as our sample, and a standard deviation equal to $\\frac{s}{\\sqrt n}$.  \n",
    "This latter fraction is called the __standard error of the mean__, or SEM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask our original question, rephrasing it as: __How much of an outlier is the *difference between our sample mean and the hypothesized mean of 30*?__  \n",
    "\n",
    "And we are now able to precisely specify how we expect this *difference* to be distributed:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'The t-distribution')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "fig,ax = pl.subplots()\n",
    "\n",
    "for n,col in zip([5,10,1000],['r','g','b']):\n",
    "    samples = np.array([np.random.choice(population, size=n) for i in range(N)])\n",
    "    normed = (samples.mean(axis=1)-mean) / (samples.std(axis=1,ddof=1)/np.sqrt(n))\n",
    "    ax.hist(normed, bins=100, histtype='step', label=f'n={n}', density=True, lw=1, color=col)\n",
    "    \n",
    "    xs = np.arange(-4,4,.01)\n",
    "    ys = t.pdf(xs, n-1)\n",
    "    ax.plot(xs,ys, color=col)\n",
    "ax.legend(fontsize='x-small')\n",
    "ax.set_xlim([-5,5])\n",
    "ax.set_title('The t-distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task__: load in the sample from above and compute a manual t-test to determine whether or not it comes from a population with a mean of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = np.load('sample.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
