{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b151b4ef-982c-4548-b486-5b31e50ef16e"
    }
   },
   "source": [
    "# neuro boot camp day 1\n",
    "## wrangling ephys data\n",
    "\n",
    "## contents\n",
    "* [1. types of data neuroscientists acquire](#data)\n",
    "* [2. voltage trace as vector (everything is a vector)](#vect)\n",
    "* [3. peak detection (from scratch)](#peak)\n",
    "* [4. simple spike sorting](#sort)\n",
    "* [5. filtering: low, high pass; notch](#filt)\n",
    "* [6. spike trains, raster plots](#plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "332e0402-c1fb-42bb-875a-7493bfce0f73"
    }
   },
   "source": [
    "## 0. preliminaries\n",
    "\n",
    "(FYI: if you are an advanced student and you breeze through these exercises, I would point you at [Neural Data Science](https://www.sciencedirect.com/book/9780128040430/neural-data-science) by Nylen and Wallisch.  You can push yourself to work through some of the more advanced examples there.  Everyone else may find it a useful set of prompts to consider down the road.)\n",
    "<br><br>\n",
    "<span style=\"color:dodgerblue\">Commentary added after the workshop appears in blue.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "5e226fb1-f3a0-497e-a45f-96c7a7ee8b84"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib tk\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "14a55ace-473c-4ce7-85c9-9e251ff044e2"
    }
   },
   "source": [
    "<a id=\"data\"></a>\n",
    "## 1. types of data neuroscientists acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "50d9edec-0e99-4aa0-a8df-4048fffcdbbb"
    }
   },
   "source": [
    "**Exercise 1**: Load dataset.npy, and tell me what's in it.\n",
    "<br><br>\n",
    "<font color=\"dodgerblue\">The purpose of this little puzzle was to demonstrate that it can be tough for the untrained eye to tell the difference between neuroscience data from different sources.  Ephys traces are vectors, but so are fMRI voxels over time, calcium imaging ROIs, etc... The idea I was trying to sell is that, since all sorts of data come in this way, your approach to analysis is generalizable at the early stages.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1600)\n"
     ]
    }
   ],
   "source": [
    "# this was a suggestion for how to look at our dataset\n",
    "\n",
    "dataset = np.load('dataset.npy')\n",
    "\n",
    "fig1, ax1 = pl.subplots(num=1) # If you are explicit about giving each figure a different handle,\n",
    "                               # you can use that handle to edit the figure/axes later.  passing\n",
    "                               # this as the argument num will sync the figure's title.    \n",
    "ax1.plot(dataset)\n",
    "\n",
    "# but it isn't plotting the way we expect:\n",
    "# we want our 1600 samples on the x axis\n",
    "\n",
    "print(np.shape(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.72529030e+01  3.72529030e+01  9.31322575e+00 ...  8.38190308e+01\n",
      "   9.31322632e+01  1.11758713e+02]\n",
      " [ 1.31100680e+02  1.21829932e+02  1.12443537e+02 ...  1.20889796e+02\n",
      "   1.09044898e+02  1.20687075e+02]\n",
      " [ 7.01904249e+00  7.93456984e+00  6.10351515e+00 ...  1.37329092e+01\n",
      "   1.40380850e+01  2.74658179e+00]\n",
      " ...\n",
      " [ 2.29492188e+00  2.83203125e+00 -1.26953125e+00 ...  1.16699219e+01\n",
      "   1.13281250e+01  6.39648438e+00]\n",
      " [ 1.07969521e+04  1.11958330e+04  1.08577852e+04 ...             nan\n",
      "              nan             nan]\n",
      " [ 1.00722529e+04  9.98243359e+03  1.03290059e+04 ...             nan\n",
      "              nan             nan]]\n"
     ]
    }
   ],
   "source": [
    "# this one successfully plots the data\n",
    "\n",
    "dataset = np.load('dataset.npy')\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "\n",
    "for x in dataset[:,0::]:\n",
    "    ax.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above example was explicit about indexing dataset for our loop.\n",
    "# we could have done it implicitly:\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "\n",
    "for x in dataset:\n",
    "    ax.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb1e07f128>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's kind of tough to see the individual plots here.  One solution is to plot them one at \n",
    "# a time.  Here's one plot, that Carlos thought was an ephys trace.  (He was right!)\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "ax.plot(dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dodgerblue>I wanted to put all the traces in one plot that accomplished two things: the traces would be scaled so that you could pick out individual features, and the traces would be separated in vertical space so you could better see each trace.\n",
    "<br><br>\n",
    "    To scale the traces, I chose to perform a <b>z-test</b>, which transforms the y scale to units in terms of standard deviation.  To formalize it, the Z score is defined as \n",
    "    </font><center>$\\Large z = \\frac{\\color{red} x - \\color{blue} \\mu}{\\color{violet} \\sigma}$</center>\n",
    "\n",
    "where $\\color{red} x$ is our sample value, $\\color{blue} \\mu$ is the population mean, and $\\color{violet} \\sigma$ is the population standard deviation.  Ben will discuss z-scoring more in the statistics module.\n",
    "<br><br><font color=dodgerblue> Z-scoring was one possible approach.  I could instead have chosen (e.g.) to scale each trace so that its min and max values were mapped to the range (-1,+1).\n",
    "    <br><br>\n",
    "    To visually separate our z-scored traces (where each is now centered on zero), I added an offset based on the index <i>i</i> as I went through the loop that did the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28727752  0.28727752  0.09853784 ...  0.60184365  0.66475692\n",
      "   0.79058336]\n",
      " [-0.04173179 -0.32455262 -0.61090144 ... -0.35323316 -0.71458304\n",
      "  -0.35941753]\n",
      " [-0.16231574 -0.12129023 -0.20334126 ...  0.13853802  0.1522132\n",
      "  -0.35376814]\n",
      " ...\n",
      " [ 0.19735814  0.23970503 -0.08367119 ...  0.93650379  0.90955577\n",
      "   0.52073436]\n",
      " [-1.02605356  0.7207951  -0.75964304 ...         nan         nan\n",
      "          nan]\n",
      " [-0.37808492 -0.82812243  0.90836914 ...         nan         nan\n",
      "          nan]]\n"
     ]
    }
   ],
   "source": [
    "# goal: scale to comparable y ranges.  method: z score\n",
    "\n",
    "z_data = dataset # initializing z_data\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    z_data[i] = (z_data[i] - np.nanmean(z_data[i])) / np.nanstd(z_data[i])\n",
    "    \n",
    "# I had originally written the above line with np.mean() and np.std() but this failed to\n",
    "# give any numbers for my last two traces (which have NaNs inside) \n",
    "    \n",
    "print(z_data)\n",
    "\n",
    "# the function enumerate allows us to use a for loop where we have both an index(i) and\n",
    "# the values of the list or array being iterated (here x).  If you find using enumerate \n",
    "# confusing, I would encourage you to write your for loops as I did above:\n",
    "# for i in range(len(...)):\n",
    "\n",
    "# I also strongly suggest that you ALWAYS use `i` to refer to an index, not the iterated value\n",
    "\n",
    "for i,x in enumerate(z_data):\n",
    "    ax.plot(x+i*3, lw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate command above is equivalent to:\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "\n",
    "i=0\n",
    "for x in z_data:\n",
    "    ax.plot(x+i*3)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and also (I'd encourage you to start out writing your loops this way):\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "\n",
    "for i in range(len(z_data)):     # note I did not need to initialize i=0 this time.\n",
    "    ax.plot(z_data[i] + i*3)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue\">For your amusement/enrichment, these were the sources of the traces: </span>\n",
    "    \n",
    "\n",
    "| source   | y value/units   |  sample interval |\n",
    "|:-:|:-:|:-:|\n",
    "|0: drosophila courship song| digitized speaker signal (µV)|  100 µsec |\n",
    "|1: calcium imaging ROI| raw fluorescence in arbitrary units| 40 msec |\n",
    "|2: crayfish extracellular nerve recording|voltage (µV)| 100 µsec|\n",
    "|3: intracellular Aplysia neuron recording|membrane potential(mV)|100 µsec|\n",
    "|4: calcium imaging ROI|raw fluorescence (arbitrary units)|40 msec|\n",
    "|5: [EEG data](http://bnci-horizon-2020.eu/database/data-sets)|µV| 4 msec|\n",
    "|6: fMRI voxels|BOLD signal (arbitrary units)| 1.5 seconds|\n",
    "|7: fMRI voxels|BOLD signal|1.5 seconds|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vect\"></a>\n",
    "## 2. voltage trace as a vector (everything is a vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue\">I didn't elaborate any examples here, but I restated the punchline I was setting up in part 1.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "744bcbd3-1dca-4308-8405-b08721f4045e"
    }
   },
   "source": [
    "<a id=\"peak\"></a>\n",
    "## 3. peak detection (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b6a4441-b8b4-4290-90a6-97e08db392ff"
    }
   },
   "source": [
    "Python has some libraries that allow for peak detection, but I think doing some manual peak finding is a useful way to hone your python skills as well as think about ephys traces.\n",
    "\n",
    "For the early parts of this exercise, we'll start with a simple sine wave.  But even this step requires a little bit of thought.  We're not going to find peaks on an abstract or analog sine wave, but rather that is explicitly sampled over time.  Play with <span style=\"color:dodgerblue\"> different sampling frequencies (**Fs**) / sample intervals</span> and sine wave frequencies (**f**) to see if you can build some intuition about how *Fs* needs to relate to *f* in order to be able to pick out individual peaks in our sine wave.  <span style=\"color:dodgerblue\">This is an exploration of the **Nyquist limit** (mentioned by Sue Ann in this morning's lecture).</span>\n",
    "\n",
    "**Exercise 3:** write a function that will find the local maxima in a sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interlude\n",
    "#\n",
    "# what should the input sinewave look like??\n",
    "\n",
    "# we did this definition with a sample interval -- \n",
    "# we could have equivalently used a sampling *frequency* instead.\n",
    "\n",
    "# I've added in an option here to pass a reference to an axis to plot onto.\n",
    "#    (I'll use it below when I do several plots)\n",
    "#\n",
    "# I also added the option to suppress plotting.\n",
    "#\n",
    "# Finally, I believe this is also the first you're seeing inline if statements.\n",
    "# Recall my discussion of my foible of wanting concise code...\n",
    "\n",
    "def give_me_a_sine(freq, si=.001, total_time=1, ax=None, plot=True):    \n",
    "    t = np.arange(0,total_time,si)\n",
    "    x = np.sin(2*np.pi*freq*t)\n",
    "    if not ax: fig,ax=pl.subplots() \n",
    "    if plot: ax.plot(t,x,'.-',lw=1) \n",
    "    return x,t # originally we only returned x, but I think you will see t is useful as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinewave,_ = give_me_a_sine(8, 0.001)  # if we don't want `t` right now, no problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the effect of different sampling rates on our generated sine curve\n",
    "\n",
    "fig_sines, ax_sines = pl.subplots()\n",
    "\n",
    "for interval in [0.005, 0.05, 0.1]:\n",
    "    give_me_a_sine(freq=8, si=interval, total_time=1, ax=ax_sines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue\">Next, before I cut you loose on developing this algorithm, we talked throught the approach at the board.  There are quite a few ways to approach this problem.  </span>\n",
    "    \n",
    "<span style=\"color:dodgerblue\">1. One of the first suggestions was to detect all local maxima -- loop over all values in your data array, and annotate it as a peak if it is larger than the surrounding two.  This will certainly work for our sine wave, but it isn't immediately generalizable to recorded data because such data are seldom smooth -- let's look a bit more closely at one of our ephys traces from above:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb21d6dd30>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = pl.subplots()\n",
    "ax.plot(dataset[2,1140:1400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue\"> For that approach to work on this trace, you'd need to either pre-processing (e.g., smooth the curve) or post-processing of the peaks detected.</span>\n",
    "\n",
    "<span style=\"color:dodgerblue\">2. A second approach that we didn't get into is to take the derivative of the curve to find inflection points.  This approach would have similar constraints to approach \\#1.</span>\n",
    "\n",
    "<span style=\"color:dodgerblue\">3. The approach we settled on implementing was to use a simple threshold to define our peaks.  If we define a threshold, then we can find a _global_ maximum in each segment we isolate.</span>\n",
    "    \n",
    "<span style=\"color:dodgerblue\">To break this in to manageable chunks, our steps are:</span>\n",
    "* define a threshold (black line)\n",
    "* iterating over values in data trace (blue dots/line):\n",
    "    * find (and record) the places where the trace crosses the threshold (red circles)\n",
    "* between each set of threshold traversals, find the maximum value\n",
    "![alt_text](peak_detect.png)\n",
    "\n",
    "(This figure is generated below (with the addition of the peaks detected).)\n",
    "\n",
    "<span style=\"color:dodgerblue\">I will implement this with two approaches: loop based and vectorized.</span>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sines, ax_sines = pl.subplots()\n",
    "sinewave, sine_times = give_me_a_sine(8, 0.004, ax=ax_sines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 39, 70, 102, 133, 164, 195, 227]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop method:  iterate through all values; make notes on threshold crossings; find maxima\n",
    "\n",
    "def find_peaks_loop(sinewave, sine_times, thresh=None, plot=False):\n",
    "    \n",
    "    # 1: if no threshold is specified, set a default of 3 SDs above the mean of our trace.\n",
    "    if thresh == None:\n",
    "        thresh = np.mean(sinewave) + 3 * np.std(sinewave)\n",
    "    \n",
    "    # 2: find threshold crossings\n",
    "    up_bounds = []\n",
    "    down_bounds = []\n",
    "    peaks = []\n",
    "    \n",
    "    for i in range(len(sinewave)-1):  # why minus 1?  What happens if you don't have that?\n",
    "        if sinewave[i]<thresh and sinewave[i+1]>thresh:\n",
    "            up_bounds.append(i)\n",
    "        if sinewave[i]>thresh and sinewave[i+1]<thresh:\n",
    "            down_bounds.append(i+1)\n",
    "        \n",
    "    # 3: find maximum between threshold crossings\n",
    "    for i in range(len(up_bounds)):   # what if there were none?  should we build in an error?\n",
    "        chunk = sinewave[up_bounds[i]:down_bounds[i]]\n",
    "        peaks.append(chunk.argmax()+up_bounds[i]) #add up_bounds[i] because this is in coord sys of chunk\n",
    "        \n",
    "    # 4: if not suppressing, plot the bounds and the peaks\n",
    "    if plot:\n",
    "        fig_sines, ax_sines = pl.subplots()\n",
    "        ax_sines.hlines(y=thresh,xmin=0,xmax=1)\n",
    "        ax_sines.plot(sine_times, sinewave, 'o-', color=\"cornflowerblue\",  linewidth=1, markersize=2)\n",
    "        ax_sines.plot(sine_times[up_bounds],sinewave[up_bounds], \n",
    "              'o', markeredgecolor=\"red\", markerfacecolor=\"None\", markersize=8, label=\"up_bounds\")\n",
    "        ax_sines.plot(sine_times[down_bounds],sinewave[down_bounds], \n",
    "              'o', markeredgecolor=\"maroon\", markerfacecolor=\"None\", markersize=8, label=\"down_bounds\")\n",
    "        ax_sines.plot(sine_times[peaks],sinewave[peaks], \n",
    "              'o', markeredgecolor=\"black\", markerfacecolor=\"None\", markersize=8, label=\"peaks\")\n",
    "        ax_sines.legend(loc=(0.7,1.05))\n",
    "        pl.tight_layout()\n",
    "        \n",
    "    return peaks  # what are we returning here?  peak indices? peak heights?\n",
    "\n",
    "find_peaks_loop(sinewave, sine_times, 0.5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li> <span style=\"color:dodgerblue\">Zoom in on the plot to evaluate whether we've captured the peaks correctly. (good to get in this habit -- even at this low sampling rate there are a lot of data points squeezed inside that circle!)</span></li>\n",
    "\n",
    "<li> <span style=\"color:dodgerblue\">In part 3, I could have gone through with a loop instead of using np.max(), but then: how do I deal with the possibility that two successive points are equal?  (This does *not* imply a peak!)  You'd have to do some kind of recursion or record keeping of sequences like that.</span></li>\n",
    "\n",
    "<li> <span style=\"color:dodgerblue\">Is there a circumstance where I would observe two peaks without dropping to the threshold between them?  (Not in this sinewave, but maybe in data we care about.)</span></li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 39, 70, 102, 133, 164, 195, 227]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorized method: use boolean indexing to pull out values above threshold, np.diff() to\n",
    "# find discontinuities between the indexes of these chunks, and then find peaks in the chunks.\n",
    "\n",
    "def find_peaks_vect(sinewave,sine_times,thresh=None, plot=False):\n",
    "\n",
    "    peaks = []\n",
    "    \n",
    "    # 1: if no threshold is specified, set a default of 3 SDs above the mean of our trace.\n",
    "    if thresh == None:\n",
    "        thresh = np.mean(sinewave) + 3 * np.std(sinewave)\n",
    "        \n",
    "    # 2: find threshold crossings\n",
    "    \n",
    "    # note: here we are only finding one boundary of the thresh traversal.\n",
    "    # we'll still find the max if we go from one bound to the next.\n",
    "\n",
    "    greater_than_threshold = sinewave > thresh\n",
    "\n",
    "    above_thresh_indices, = np.where(greater_than_threshold)  \n",
    "    # comma here is because np.where returns a tuple. equivalent:\n",
    "    # indices = np.where(greater_than_threshold)[0]\n",
    "    \n",
    "    discontinuities = np.diff(above_thresh_indices)\n",
    "\n",
    "    discont_indices = above_thresh_indices[np.where(discontinuities!=1)[0]]\n",
    "    \n",
    "    discont_indices = np.concatenate([[0], discont_indices, [above_thresh_indices[-1]]])\n",
    "        # we need to add 0 and the last value to capture all of the chunks.\n",
    "    \n",
    "    \n",
    "    # 3: find peaks\n",
    "\n",
    "    #we tried but were unable to come up with a straightforward way to do a fully vectorized implementation\n",
    "    \n",
    "    for i in range(len(discont_indices)-1):\n",
    "        chunk = sinewave[discont_indices[i]:discont_indices[i+1]]\n",
    "        peaks.append(chunk.argmax()+discont_indices[i]) \n",
    "    \n",
    "        \n",
    "    # 4: if not suppressing, plot the bounds and the peaks\n",
    "    if plot:\n",
    "        fig_sines, ax_sines = pl.subplots()\n",
    "\n",
    "        ax_sines.plot(sine_times[above_thresh_indices],sinewave[above_thresh_indices], \n",
    "              'o', color=\"yellow\", markersize=8, label='above_thresh')  #we want this one in the back\n",
    "        \n",
    "        ax_sines.hlines(y=thresh,xmin=0,xmax=1)\n",
    "        ax_sines.plot(sine_times, sinewave, 'bo-', color=\"steelblue\", lw=1, markersize=3)\n",
    "\n",
    "        ax_sines.plot(sine_times[discont_indices],sinewave[discont_indices], \n",
    "              'o', markeredgecolor=\"red\", markerfacecolor=\"None\", markersize=8, label='discontinuities')\n",
    "        ax_sines.plot(sine_times[peaks],sinewave[peaks], \n",
    "              'o', markeredgecolor=\"black\", markerfacecolor=\"None\", markersize=8, label='peaks')\n",
    "        ax_sines.legend(loc=(0.7,1.05))\n",
    "        pl.tight_layout()\n",
    "\n",
    "    return peaks\n",
    "        \n",
    "find_peaks_vect(sinewave, sine_times, 0.5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 39, 70, 102, 133, 164, 195, 227]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third implementation: this was the one that I deployed quickly when the peakutils package\n",
    "# choked on our actual neural data.  It is adapted from code written by Ben.\n",
    "\n",
    "def find_peaks_ben(data, times, thresh=None, plot=False, direction=1):\n",
    "    \n",
    "    #new argument: `direction` = 1 for upward going; -1 for downward going\n",
    "    \n",
    "    from scipy.ndimage import label\n",
    "    \n",
    "    peaks = []\n",
    "    chunks = []\n",
    "    \n",
    "    # 1: if no threshold is specified, set a default of 3 SDs above the mean of our trace.\n",
    "    if thresh == None:\n",
    "        thresh = data.mean() + data.std() * 3\n",
    "        \n",
    "    # 2: find the peaks\n",
    "    \n",
    "    above_thresh = direction * data > thresh\n",
    "\n",
    "    # split the above_thresh signal into connected segments\n",
    "    # this is a new function -- if you're curious about what it returns, print it!\n",
    "    labs,nlabs = label(above_thresh)\n",
    "    \n",
    "    for chunk in range(1,nlabs+1):        # iterate through each segment          \n",
    "        idxs = np.where(labs==chunk)[0]   # find the indices of this segment            \n",
    "        vals = data[idxs]                 # extract the signal values at these idxs    \n",
    "        peak_idx = idxs[np.argmax(vals)]  # select the index of peak signal value    \n",
    "        peaks.append(peak_idx)\n",
    "        chunks.append(idxs)\n",
    "\n",
    "    # 3: if not suppressing, plot the bounds and the peaks\n",
    "    if plot:\n",
    "        fig_sines, ax_sines = pl.subplots()\n",
    "\n",
    "        ax_sines.plot(times, sinewave, 'bo-', color=\"steelblue\", lw=1, markersize=3)\n",
    "        ax_sines.hlines(y=thresh,xmin=0,xmax=1)\n",
    "\n",
    "        cplot=[]\n",
    "        for chunk in chunks:\n",
    "            handle, = ax_sines.plot(times[chunk], data[chunk], \"o-\", lw=2, markersize=4)\n",
    "            cplot.append(handle)\n",
    "            \n",
    "        pplot, = ax_sines.plot(times[peaks],sinewave[peaks], \n",
    "              'o', markeredgecolor=\"black\", markerfacecolor=\"None\", markersize=8)\n",
    "          \n",
    "        ax_sines.legend([cplot[0],pplot],[\"chunks\",\"peaks\"],loc=(0.7,1.05))\n",
    "        pl.tight_layout()\n",
    "\n",
    "    return peaks\n",
    "\n",
    "find_peaks_ben(sinewave, sine_times, 0.5, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ffd425fb-8e59-4768-9cbe-fe514d986ecd"
    }
   },
   "source": [
    "<span style=\"color:dodgerblue\">The above three functions were all implementations of a threshold-based peak detection.  I mentioned a few other approaches above, and each of those can be expanded by consideration of other parameters, like the allowable distance between peaks, prominence, ... For reference, see the [matlab findpeaks documentation](http://www.mathworks.com/help/signal/ref/findpeaks.html) and their [peak finding tutorial](https://www.mathworks.com/help/signal/examples/peak-analysis.html).</span>\n",
    "![alt text](http://www.mathworks.com/help/examples/signal/win64/DeterminePeakWidthsExample_02.png \"peak features\")\n",
    "    \n",
    "<span style=\"color:dodgerblue\">Next I proposed to extend our use of this function to actual ephys data.  Will these functions work on one of the traces in _dataset.npy_?  What additional concerns prop up?</span>\n",
    "\n",
    "<span style=\"color:dodgerblue\">To expediate matters, I attempted to do this using the [peakutils](https://bitbucket.org/lucashnegri/peakutils) package, but this crashed when I tried to feed it actual ephys data.  Here's are a few more resources in that direction:</span>\n",
    "* a [blog post](https://blog.ytotech.com/2015/11/01/findpeaks-in-python/) discussing various packages containing a peak finding function;\n",
    "* a [jupyter notebook](http://nbviewer.jupyter.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb) that was the basis for the peakutils package.  The peak detection function is listed inside the notebook if you want to see how the approach compares to what we came up with.\n",
    "* there's also a [built-in scipy function](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.signal.find_peaks_cwt.html) based on wavelets, a different approach that lacks the features we were seeking to build out from the matlab model.\n",
    "\n",
    "<span style=\"color:dodgerblue\">To load in ephys data from its native binary file, I installed the package [neo](https://neo.readthedocs.io/en/0.6/), by typing <code>pip install neo</code> from a terminal window.  Neo has functions well beyond file IO if you are interested.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using package: neo\n",
    "\n",
    "r = neo.AxonIO(filename='sample3.abf') # this block is just stepping into the data structure\n",
    "block = r.read_block()\n",
    "segment = block.segments[0]\n",
    "data = segment.analogsignals[0]\n",
    "# There are actually two channels in this data. Can you pull out the second one?\n",
    "\n",
    "# `data` will be most useful to us as a 1-d array.\n",
    "data = np.asarray(data).flatten()\n",
    "\n",
    "# generate time array.  this can alternatively be pulled out of the neo data struct.\n",
    "# 1e-4 is our sample interval expressed in seconds.\n",
    "times = np.arange(len(data))*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run our collection of peak finders\n",
    "\n",
    "loop_peaks = find_peaks_loop(data, times)\n",
    "vect_peaks = find_peaks_vect(data, times)\n",
    "ben_peaks = find_peaks_ben(data, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look.\n",
    "\n",
    "ephys_plot, ephys_ax = pl.subplots()\n",
    "ephys_ax.plot(times, data, '-', color=\"cornflowerblue\",  lw=1, markersize=2)\n",
    "ephys_ax.plot(times[loop_peaks], data[loop_peaks], \n",
    "              'o', markeredgecolor=\"black\", markerfacecolor=\"None\", markersize=8, label='loop')\n",
    "ephys_ax.plot(times[vect_peaks], data[vect_peaks], \n",
    "              'd', markeredgecolor=\"chartreuse\", markerfacecolor=\"None\", markersize=8, label='vect')\n",
    "ephys_ax.plot(times[ben_peaks], data[ben_peaks], \n",
    "              's', markeredgecolor=\"magenta\", markerfacecolor=\"None\", markersize=8, label='ben')\n",
    "ephys_ax.legend(loc=(0.7,1.05))\n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue\">**How do our functions do with real data?**  Zoom in and take a close look.  The vector-based peak detection seems to be detecting extra peaks.  Can you figure out why?  (Hint/aside: I built out the remainder of your solutions using the built-in plot functions to have a visual record of what was being detected at each step, like a print debug statement.  You can likewise use these debugging plots to help troubleshoot here.)  Did we choose a good default threshold?</span>\n",
    "\n",
    "<span style=\"color:magenta\">**Disclaimer!** this code as implemented/called is not fully up to the task of handling (even our sample) ephys data.  Part of your pipeline for any analysis would be to tune your functions to handle the particular data you recorded. \n",
    "\n",
    "<span style=\"color:dodgerblue\">Next I ran through some examples of what you can start to do with your detected peaks.</span>\n",
    "\n",
    "**Exercise:** show me the average waveform for one class of detected spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0: let's pick one set of peaks to use for further analysis\n",
    "\n",
    "peak_idxs = loop_peaks\n",
    "peak_heights = data[peak_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 85.,  68., 458.,   3.,   8., 255.,  26.,   0.,   0.,   1.]),\n",
       " array([ 69.58007812, 101.53197937, 133.48388062, 165.43578186,\n",
       "        197.38768311, 229.33958435, 261.2914856 , 293.24338684,\n",
       "        325.19528809, 357.14718933, 389.09909058]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: plot a histogram\n",
    "\n",
    "fig_hist, ax_hist = pl.subplots()\n",
    "ax_hist.hist(peak_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 68., 178.,  42.,   1.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([223.6938324 , 240.23435822, 256.77488403, 273.31540985,\n",
       "        289.85593567, 306.39646149, 322.9369873 , 339.47751312,\n",
       "        356.01803894, 372.55856476, 389.09909058]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: pull out 1 discrete class of AP that you like from your histogram\n",
    "# (make a list of the peaks from that class)\n",
    "\n",
    "peak_idxs_ary = np.array(peak_idxs)\n",
    "peak_heights_ary = np.array(peak_heights).flatten()\n",
    "\n",
    "\n",
    "selected_peak_indices = peak_idxs_ary[peak_heights_ary>200]\n",
    "ax_hist.hist(data[selected_peak_indices])\n",
    "#print(selected_peak_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2c67eef0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3: take a window surrounding each detected peak and store it in a list\n",
    "# step 4: plot the average waveform of this class of peaks\n",
    "\n",
    "#data=data.flatten()\n",
    "waveforms = []\n",
    "for peaks in selected_peak_indices:\n",
    "    waveforms.append(data[peaks-20:peaks+30])\n",
    "    \n",
    "waveform_ary = np.array(waveforms)\n",
    "\n",
    "fig_wave, ax_wave = pl.subplots()\n",
    "ax_wave.plot(waveform_ary.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1a2b91a358>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 5: plot a set of extremities for the waveform (min/max, SEM, or std dev)\n",
    "ax_wave.fill_between(range(waveform_ary.shape[1]),\n",
    "                  waveform_ary.mean(axis=0)-waveform_ary.std(axis=0),\n",
    "                  waveform_ary.mean(axis=0)+waveform_ary.std(axis=0),\n",
    "                  color='teal', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sort\"></a>\n",
    "## 4. simple spike sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue\">Steps 1 and 2 illustrate a rudimentary form of spike sorting.  Illustrated below is a slightly more systematic approach, using k-means clustering to separate spikes based on amplitude.  This has a number of limitations, including that it will dutifully generate as many clusters as we ask for (which we may not have an *a priori* estimate of) and that it will generate clusters with as much separation as possible (not necessarily a safe assumption that your spike classes should be separable in this way).</span>\n",
    "\n",
    "<span style=\"color:dodgerblue\">There's a rich literature on spike sorting with different approaches, including PCA, which we'll touch on briefly in Friday's class</span>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5\n",
    "\n",
    "# reshape the data to the shape (n_samples, n_features) -- required for scikit-learn\n",
    "X = peak_heights.reshape([-1,1])\n",
    "# run k-means clustering\n",
    "km = KMeans(n_clusters=n_clusters).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a2de39128>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the nerve and the peaks colored by cluster\n",
    "fig_clus, ax_clus = pl.subplots()\n",
    "ax_clus.plot(data, color='gray', lw=1)\n",
    "ax_clus.scatter(peak_idxs, peak_heights, c=km.labels_, s=20, zorder=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For future reference: play with https://github.com/tridesclous/tridesclous.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d24e11f2-8b53-4b81-962f-08db7a941dcc"
    }
   },
   "source": [
    "<a id=\"filt\"></a>\n",
    "## 5. filtering: low, high pass; notch; baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cb77c04e-4f1b-4059-98b8-70551faa819a"
    }
   },
   "source": [
    "<span style=\"color:dodgerblue\">We did not make it to discussion of filtering, but there are many ways that filtering are relevant to processing ephys data.  Analog traces are often prefiltered -- high and low pass filters can separate an electrode recording into a spiketrain (from high pass filtering) and a local field potential (low pass filtering).  Sometimes it's necessary to apply a notch filter to remove AC noise (i.e. filter out 60 ± 0.5 Hz signal).  Drifty data can be baseline filtered to make things like peak detection much easier.  The figure below is from <u>NDS</u> (Nylen and Wallisch).</span>\n",
    "\n",
    "![alt_text](filtering.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "836ebbd1-f9aa-4521-9385-8528cd04036c"
    }
   },
   "source": [
    "<a id=\"plot\"></a>\n",
    "## 6. spike trains, raster plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue\">One way to represent the information encoded by neurons is to record just a binary stream of spike times (ignoring spike waveforms and other features).  Spiketrains are useful in comparisons of neuronal activity at the network level (cf. information theory and population analysis discussion on Friday), and are often displayed as raster plots.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbpresent": {
     "id": "a6313cea-5fe1-4b25-837d-abb3af62fd4a"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 20, 24, 25, 41, 74, 84, 85, 97]\n"
     ]
    }
   ],
   "source": [
    "# a simulation of a raster plot\n",
    "\n",
    "A = np.random.choice([0,1], 1000, p=[0.9,0.1]).reshape(10,100)\n",
    "\n",
    "fig6,ax6 = pl.subplots()\n",
    "\n",
    "spiketimes = [i for i,x in enumerate(A[0]) if x==1]\n",
    "ax6 = pl.vlines(spiketimes,0,1)\n",
    "\n",
    "print(spiketimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exercise 6**: show me a plot with 10 spiketrain rasters in it, that looks like this:\n",
    "\n",
    "![alt_text](Figure_1.png)\n",
    "\n",
    "**exercise 6 part 2**: repeat exercise 6, starting with an ephys trace.\n",
    "\n",
    "**sidequest**:  Can you use vlines to put color-coded y scales correlated with each trace in that first plot?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {
    "0d418e11-6889-4303-a070-950c46ce2390": {
     "id": "0d418e11-6889-4303-a070-950c46ce2390",
     "prev": "9ae4fde1-559d-4e25-ad23-30bedd5b9bd8",
     "regions": {
      "044fa389-1e0e-4085-b019-b3324689d0a6": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "044fa389-1e0e-4085-b019-b3324689d0a6"
      }
     }
    },
    "66c26ec2-77d5-4bc8-be7a-f979e158419d": {
     "id": "66c26ec2-77d5-4bc8-be7a-f979e158419d",
     "prev": "886e381e-95cf-4b14-9568-25ebb1a8fc75",
     "regions": {
      "8e210a30-cedc-4dc0-b1f2-1e3153179aa1": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "8e210a30-cedc-4dc0-b1f2-1e3153179aa1"
      }
     }
    },
    "886e381e-95cf-4b14-9568-25ebb1a8fc75": {
     "id": "886e381e-95cf-4b14-9568-25ebb1a8fc75",
     "prev": null,
     "regions": {
      "20a07646-1588-47f2-81d8-4042915872d2": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "20a07646-1588-47f2-81d8-4042915872d2"
      }
     }
    },
    "9ae4fde1-559d-4e25-ad23-30bedd5b9bd8": {
     "id": "9ae4fde1-559d-4e25-ad23-30bedd5b9bd8",
     "prev": "66c26ec2-77d5-4bc8-be7a-f979e158419d",
     "regions": {
      "a4cdc7a3-402b-4689-b0ba-97b34b1d6ebd": {
       "attrs": {
        "height": 0.6,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "id": "a4cdc7a3-402b-4689-b0ba-97b34b1d6ebd"
      },
      "b424d741-9b2c-4e1d-969c-28501e4ea276": {
       "attrs": {
        "height": 0.2,
        "width": 0.4,
        "x": 0.5,
        "y": 0.7
       },
       "id": "b424d741-9b2c-4e1d-969c-28501e4ea276"
      },
      "e3ff6259-fbcf-4ac7-a74c-e4dd56e2fb87": {
       "attrs": {
        "height": 0.2,
        "width": 0.4,
        "x": 0.1,
        "y": 0.7
       },
       "id": "e3ff6259-fbcf-4ac7-a74c-e4dd56e2fb87"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
